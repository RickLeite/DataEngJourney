{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/09 15:43:35 WARN Utils: Your hostname, wedivv-H110M-S2V resolves to a loopback address: 127.0.1.1; using 192.168.1.44 instead (on interface wlp5s0)\n",
      "23/05/09 15:43:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/09 15:43:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/09 15:43:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/09 15:43:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "| Extrato Conta Corrente |\n",
      "+------------------------+\n",
      "|                  Conta |\n",
      "|                Período |\n",
      "|         Data Lançamento|\n",
      "|              17/03/2023|\n",
      "|              13/03/2023|\n",
      "+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('./data/1-Extrato.csv', header=True, sep=';').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---+\n",
      "| Extrato Conta Corrente | id|\n",
      "+------------------------+---+\n",
      "|                  Conta |  0|\n",
      "|                Período |  1|\n",
      "|         Data Lançamento|  2|\n",
      "|              17/03/2023|  3|\n",
      "|              13/03/2023|  4|\n",
      "+------------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \";\")\n",
    "    .load(\"./data/1-Extrato.csv\")\n",
    ")\n",
    "\n",
    "df_with_id = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "df_with_id.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---+\n",
      "| Extrato Conta Corrente | id|\n",
      "+------------------------+---+\n",
      "|              17/03/2023|  3|\n",
      "|              13/03/2023|  4|\n",
      "|              13/03/2023|  5|\n",
      "|              07/12/2022|  6|\n",
      "|              06/12/2022|  7|\n",
      "+------------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df_with_id.filter(df_with_id.id >= 3)\n",
    "df_filtered.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "monotonically_increasing_id() does not guarantee consecutive values. </br>\n",
    "Using this to filter out the header rows with condition like id > 2 does not ensure it is the lines that you want to be selected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Extrato Conta Corrente ',\n",
       " 'Conta ;1000-0',\n",
       " 'Período ;01/01/2022 a 27/04/2023',\n",
       " '',\n",
       " 'Data Lançamento;Histórico;Descrição;Valor;Saldo',\n",
       " '17/03/2023;Pix enviado ;Growth Supplements P A E;-116,25',\n",
       " '13/03/2023;Cashback;Google Play;0,45',\n",
       " '13/03/2023;Compra de Giftcard;;-15,00',\n",
       " '07/12/2022;Estorno;Compra cartão;11,93',\n",
       " '06/12/2022;Compra no débito;Uber   *uber   *trip   Sao Paulo     Bra;-11,93',\n",
       " '28/11/2022;Compra no débito;Uber   *uber   *trip   Sao Paulo     Bra;-24,97',\n",
       " '11/11/2022;Compra no débito;Pezao Bar              Indaiatuba    Bra;-59,25',\n",
       " '19/10/2022;Compra no débito;Uber        *trip      Sao Paulo     Bra;-7,77',\n",
       " '19/10/2022;Compra no débito;Uber   *uber   *trip   Sao Paulo     Bra;-8,90',\n",
       " '11/10/2022;Pix enviado ;Fini Comercializadora Ltda;-31,78',\n",
       " '24/09/2022;Compra no débito;Uber   *uber   *trip   Sao Paulo     Bra;-11,68',\n",
       " '19/08/2022;Compra no débito;Bullguer Sao Paulo Bra;-63,00',\n",
       " '19/08/2022;Estorno;Compra cartão;60,00',\n",
       " '19/08/2022;Compra no débito;Bullguer Sao Paulo Bra;-60,00',\n",
       " '15/08/2022;Pagamento efetuado;GROWTH SUPPLEMENTS;-126,64']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "rdd = sc.textFile(\"./data/1-Extrato.csv\")\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip the first 4 lines\n",
    "header = rdd.zipWithIndex().filter(lambda x: x[1] >= 5).keys()\n",
    "\n",
    "split_lines = header.map(lambda x: x.split(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['17/03/2023', 'Pix enviado ', 'Growth Supplements P A E', '-116,25'],\n",
       " ['13/03/2023', 'Cashback', 'Google Play', '0,45'],\n",
       " ['13/03/2023', 'Compra de Giftcard', '', '-15,00']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_lines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+--------------------+-------+\n",
      "|data_lancamento|         historico|           descricao|  valor|\n",
      "+---------------+------------------+--------------------+-------+\n",
      "|     17/03/2023|      Pix enviado |Growth Supplement...|-116,25|\n",
      "|     13/03/2023|          Cashback|         Google Play|   0,45|\n",
      "|     13/03/2023|Compra de Giftcard|                    | -15,00|\n",
      "|     07/12/2022|           Estorno|       Compra cartão|  11,93|\n",
      "|     06/12/2022|  Compra no débito|Uber   *uber   *t...| -11,93|\n",
      "+---------------+------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"data_lancamento\", StringType(), True),\n",
    "    StructField(\"historico\", StringType(), True),\n",
    "    StructField(\"descricao\", StringType(), True),\n",
    "    StructField(\"valor\", StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(split_lines, schema)\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the real dataset I have one columns that doesn't have values.\n",
    "so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "rdd = sc.textFile(\"./data/1-Extrato.csv\")\n",
    "\n",
    "# skip the first 4 lines\n",
    "header = rdd.zipWithIndex().filter(lambda x: x[1] >= 5).keys()\n",
    "\n",
    "split_lines = header.map(lambda x: x.split(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+--------------------+-------+-----+\n",
      "|data_lancamento|         historico|           descricao|  valor|saldo|\n",
      "+---------------+------------------+--------------------+-------+-----+\n",
      "|     17/03/2023|      Pix enviado |Growth Supplement...|-116,25| null|\n",
      "|     13/03/2023|          Cashback|         Google Play|   0,45| null|\n",
      "|     13/03/2023|Compra de Giftcard|                    | -15,00| null|\n",
      "|     07/12/2022|           Estorno|       Compra cartão|  11,93| null|\n",
      "|     06/12/2022|  Compra no débito|Uber   *uber   *t...| -11,93| null|\n",
      "+---------------+------------------+--------------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"data_lancamento\", StringType(), True),\n",
    "    StructField(\"historico\", StringType(), True),\n",
    "    StructField(\"descricao\", StringType(), True),\n",
    "    StructField(\"valor\", StringType(), True),\n",
    "    StructField(\"saldo\", StringType(), True)\n",
    "])\n",
    "\n",
    "# create the DataFrame with null for missing fields\n",
    "df = split_lines.map(lambda x: tuple(x + [None]*(len(schema.fields)-len(x))))\n",
    "df = spark.createDataFrame(df, schema)\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DANGER! it will read everyline to apply the null's"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
