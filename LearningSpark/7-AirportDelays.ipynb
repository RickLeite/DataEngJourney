{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/13 22:27:04 WARN Utils: Your hostname, wedivv-H110M-S2V resolves to a loopback address: 127.0.1.1; using 192.168.1.44 instead (on interface wlp5s0)\n",
      "23/05/13 22:27:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/13 22:27:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/13 22:27:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Airport performance delays\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = (\"./data/7-departuredelays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"date STRING, delay INT, distance INT, origin STRING, destination STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "    .schema(schema)\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(csvFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flights whose distance is greater than 1,000 miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-----+\n",
      "|distance|origin|destination|delay|\n",
      "+--------+------+-----------+-----+\n",
      "|    4330|   HNL|        JFK|  932|\n",
      "|    4330|   JFK|        HNL|  922|\n",
      "|    4330|   JFK|        HNL|  784|\n",
      "|    4330|   JFK|        HNL|  175|\n",
      "|    4330|   JFK|        HNL|  134|\n",
      "|    4330|   JFK|        HNL|  123|\n",
      "|    4330|   JFK|        HNL|  118|\n",
      "|    4330|   HNL|        JFK|  115|\n",
      "|    4330|   JFK|        HNL|  111|\n",
      "|    4330|   HNL|        JFK|  110|\n",
      "+--------+------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT distance, origin, destination, delay\n",
    "    FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "    ORDER BY distance DESC, delay DESC\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-----+\n",
      "|distance|origin|destination|delay|\n",
      "+--------+------+-----------+-----+\n",
      "|    4330|   HNL|        JFK|  932|\n",
      "|    4330|   JFK|        HNL|  922|\n",
      "|    4330|   JFK|        HNL|  784|\n",
      "|    4330|   JFK|        HNL|  175|\n",
      "|    4330|   JFK|        HNL|  134|\n",
      "|    4330|   JFK|        HNL|  123|\n",
      "|    4330|   JFK|        HNL|  118|\n",
      "|    4330|   HNL|        JFK|  115|\n",
      "|    4330|   JFK|        HNL|  111|\n",
      "|    4330|   HNL|        JFK|  110|\n",
      "+--------+------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "(   \n",
    "  df\n",
    "    .select(\"distance\", \"origin\", \"destination\", \"delay\")\n",
    "    .where(col(\"distance\") > 1000)\n",
    "    .orderBy(desc(\"distance\"), desc(\"delay\"))\n",
    ").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flights between San Francisco (SFO) and Chicago\n",
    "(ORD) with at least a two-hour delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-----+\n",
      "|    date|origin|destination|delay|\n",
      "+--------+------+-----------+-----+\n",
      "|02190925|   SFO|        ORD| 1638|\n",
      "|01031755|   SFO|        ORD|  396|\n",
      "|01022330|   SFO|        ORD|  326|\n",
      "|01051205|   SFO|        ORD|  320|\n",
      "|01190925|   SFO|        ORD|  297|\n",
      "|02171115|   SFO|        ORD|  296|\n",
      "|01071040|   SFO|        ORD|  279|\n",
      "|01051550|   SFO|        ORD|  274|\n",
      "|03120730|   SFO|        ORD|  266|\n",
      "|01261104|   SFO|        ORD|  258|\n",
      "+--------+------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT date, origin, destination, delay\n",
    "    FROM us_delay_flights_tbl\n",
    "    WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "    ORDER BY delay DESC\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-----+\n",
      "|    date|origin|destination|delay|\n",
      "+--------+------+-----------+-----+\n",
      "|02190925|   SFO|        ORD| 1638|\n",
      "|01031755|   SFO|        ORD|  396|\n",
      "|01022330|   SFO|        ORD|  326|\n",
      "|01051205|   SFO|        ORD|  320|\n",
      "|01190925|   SFO|        ORD|  297|\n",
      "+--------+------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .select(\"date\", \"origin\", \"destination\", \"delay\")\n",
    "    .where((col(\"delay\") > 120) & (col(\"origin\") == 'SFO') & (col(\"destination\") == 'ORD'))\n",
    "    .orderBy(desc(\"delay\"))\n",
    ").show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the date column into a readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|       date|\n",
      "+-----------+\n",
      "|01-01 12:45|\n",
      "|01-02 06:00|\n",
      "|01-02 12:45|\n",
      "+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT date_format(from_unixtime(unix_timestamp(date, 'MMddHHmm')), 'MM-dd HH:mm') as date\n",
    "FROM us_delay_flights_tbl\n",
    "\"\"\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+------+-----------+\n",
      "|       date|delay|distance|origin|destination|\n",
      "+-----------+-----+--------+------+-----------+\n",
      "|01-01 12:45|    6|     602|   ABE|        ATL|\n",
      "|01-02 06:00|   -8|     369|   ABE|        DTW|\n",
      "|01-02 12:45|   -2|     602|   ABE|        ATL|\n",
      "+-----------+-----+--------+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, from_unixtime, unix_timestamp\n",
    "\n",
    "df_updated = (\n",
    "    df.select(\n",
    "        date_format(\n",
    "            from_unixtime(\n",
    "            unix_timestamp('date', 'MMddHHmm')), 'MM-dd HH:mm').alias('date'), \n",
    "    *df.columns).drop(df.date) # organizing columns like a pro in: ../others/2-Updating And OrganizingColumns.ipynb\n",
    ") \n",
    "df_updated.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.createOrReplaceTempView(\"us_delay_flights_tbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------+-----------+------+-------------+\n",
      "|       date|delay|origin|destination|winter|      holiday|\n",
      "+-----------+-----+------+-----------+------+-------------+\n",
      "|01-01 12:37|  122|   SFO|        ORD|Winter|New Years Day|\n",
      "|01-01 14:10|  124|   SFO|        ORD|Winter|New Years Day|\n",
      "|01-02 07:20|  145|   SFO|        ORD|Winter|         null|\n",
      "|01-02 12:05|  154|   SFO|        ORD|Winter|         null|\n",
      "|01-02 14:10|  190|   SFO|        ORD|Winter|         null|\n",
      "+-----------+-----+------+-----------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  date,  delay,  origin,  destination,\n",
    "  CASE\n",
    "    WHEN SUBSTR(date, 1, 2) IN ('12', '01', '02') THEN 'Winter'\n",
    "    ELSE 'Not Winter'\n",
    "  END AS winter,\n",
    "  CASE\n",
    "    WHEN SUBSTR(date, 1, 2) = '01' AND SUBSTR(date, 4, 2) = '01' THEN 'New Years Day'\n",
    "    WHEN SUBSTR(date, 1, 2) = '07' AND SUBSTR(date, 4, 2) = '04' THEN 'Independence Day'\n",
    "    WHEN SUBSTR(date, 1, 2) = '12' AND SUBSTR(date, 4, 2) = '25' THEN 'Christmas Day'\n",
    "    ELSE NULL\n",
    "  END AS holiday\n",
    "FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND origin = 'SFO' AND destination = 'ORD'\n",
    "ORDER BY date ASC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were the delays related to\n",
    "winter months or holidays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------+-------------+\n",
      "|    winter|      holiday|num_flights|average_delay|\n",
      "+----------+-------------+-----------+-------------+\n",
      "|    Winter|         null|         37|       234.19|\n",
      "|    Winter|New Years Day|          2|        123.0|\n",
      "|Not Winter|         null|         16|       168.69|\n",
      "+----------+-------------+-----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT winter, holiday, COUNT(*) as num_flights, ROUND(AVG(delay),2) as average_delay\n",
    "    FROM (\n",
    "SELECT\n",
    "  date,  delay,  origin,  destination,\n",
    "  CASE\n",
    "    WHEN SUBSTR(date, 1, 2) IN ('12', '01', '02') THEN 'Winter'\n",
    "    ELSE 'Not Winter'\n",
    "  END AS winter,\n",
    "  CASE\n",
    "    WHEN SUBSTR(date, 1, 2) = '01' AND SUBSTR(date, 4, 2) = '01' THEN 'New Years Day'\n",
    "    WHEN SUBSTR(date, 1, 2) = '07' AND SUBSTR(date, 4, 2) = '04' THEN 'Independence Day'\n",
    "    WHEN SUBSTR(date, 1, 2) = '12' AND SUBSTR(date, 4, 2) = '25' THEN 'Christmas Day'\n",
    "    ELSE NULL\n",
    "  END AS holiday\n",
    "FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND origin = 'SFO' AND destination = 'ORD'\n",
    "ORDER BY date ASC\n",
    "    )\n",
    "    GROUP BY holiday, winter\n",
    "\"\"\").show(5)\n",
    "\n",
    "# Look like when it is winter the average delay tends to double.\n",
    "# Not enough data flights for holidays (and the data only covers from january to march)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT delay, origin, destination,\n",
    "        CASE\n",
    "            WHEN delay > 360 THEN 'Very Long Delays'\n",
    "            WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "            WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "            WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "            WHEN delay = 0 THEN 'No Delays'\n",
    "        ELSE 'Early'\n",
    "        END AS Flight_Delays\n",
    "    FROM us_delay_flights_tbl\n",
    "    ORDER BY origin, delay DESC\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+----------------+\n",
      "|delay|origin|destination|  delay_category|\n",
      "+-----+------+-----------+----------------+\n",
      "|    6|   ABE|        ATL|Tolerable Delays|\n",
      "|   -8|   ABE|        DTW| Early Departure|\n",
      "|   -2|   ABE|        ATL| Early Departure|\n",
      "|   -4|   ABE|        ATL| Early Departure|\n",
      "|   -4|   ABE|        ATL| Early Departure|\n",
      "|    0|   ABE|        ATL|       No Delays|\n",
      "|   10|   ABE|        ATL|Tolerable Delays|\n",
      "|   28|   ABE|        ATL|Tolerable Delays|\n",
      "|   88|   ABE|        ATL|    Short Delays|\n",
      "|    9|   ABE|        ATL|Tolerable Delays|\n",
      "|   -6|   ABE|        ATL| Early Departure|\n",
      "|   69|   ABE|        ATL|    Short Delays|\n",
      "|    0|   ABE|        DTW|       No Delays|\n",
      "|   -3|   ABE|        ATL| Early Departure|\n",
      "|    0|   ABE|        DTW|       No Delays|\n",
      "|    0|   ABE|        ATL|       No Delays|\n",
      "|    0|   ABE|        DTW|       No Delays|\n",
      "|    0|   ABE|        ATL|       No Delays|\n",
      "|    0|   ABE|        ORD|       No Delays|\n",
      "|    0|   ABE|        DTW|       No Delays|\n",
      "+-----+------+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "(\n",
    "    df_updated\n",
    "    .select(\n",
    "        \"delay\", \"origin\", \"destination\",\n",
    "        when(col(\"delay\") > 360, \"Very Long Delays\")\n",
    "        .when(col(\"delay\") > 120, \"Long Delays\")\n",
    "        .when(col(\"delay\") > 60, \"Short Delays\")\n",
    "        .when(col(\"delay\") > 0, \"Tolerable Delays\")\n",
    "        .when(col(\"delay\") == 0, \"No Delays\")\n",
    "        .when(col(\"delay\") < 0, \"Early Departure\")\n",
    "        .otherwise(\"Unknown\").alias(\"delay_category\")\n",
    "    )\n",
    ").show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full fly info, joining with airport codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|City      |State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|BC   |Canada |YXX |\n",
      "|Aberdeen  |SD   |USA    |ABR |\n",
      "|Abilene   |TX   |USA    |ABI |\n",
      "|Akron     |OH   |USA    |CAK |\n",
      "|Alamosa   |CO   |USA    |ALS |\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports = spark.read \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"./data/7-airport-codes-na.txt\")\n",
    "    \n",
    "airports.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+------+-----------+-----------+----------------+------------+-----------------+--------------+-------------------+\n",
      "|       date|delay|distance|origin|destination|origin_city|destination_city|origin_state|destination_state|origin_country|destination_country|\n",
      "+-----------+-----+--------+------+-----------+-----------+----------------+------------+-----------------+--------------+-------------------+\n",
      "|01-01 12:45|    6|     602|   ABE|        ATL|  Allentown|         Atlanta|          PA|               GA|           USA|                USA|\n",
      "|01-02 06:00|   -8|     369|   ABE|        DTW|  Allentown|         Detroit|          PA|               MI|           USA|                USA|\n",
      "|01-02 12:45|   -2|     602|   ABE|        ATL|  Allentown|         Atlanta|          PA|               GA|           USA|                USA|\n",
      "|01-02 06:05|   -4|     602|   ABE|        ATL|  Allentown|         Atlanta|          PA|               GA|           USA|                USA|\n",
      "|01-03 12:45|   -4|     602|   ABE|        ATL|  Allentown|         Atlanta|          PA|               GA|           USA|                USA|\n",
      "+-----------+-----+--------+------+-----------+-----------+----------------+------------+-----------------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = (\n",
    "    df_updated.alias(\"o\")\n",
    "        .join(airports.alias(\"ori\"), (col(\"o.origin\") == col(\"ori.IATA\")), \"left\")\n",
    "        .join(airports.alias(\"dest\"), (col(\"o.destination\") == col(\"dest.IATA\")), \"left\")\n",
    "    .selectExpr(\n",
    "        \"o.date\",\n",
    "        \"o.delay\",\n",
    "        \"o.distance\",\n",
    "        \"o.origin\",\n",
    "        \"o.destination\",\n",
    "        \"ori.City as origin_city\",\n",
    "        \"dest.City as destination_city\",\n",
    "        \"ori.State as origin_state\",\n",
    "        \"dest.State as destination_state\",\n",
    "        \"ori.Country as origin_country\",\n",
    "        \"dest.Country as destination_country\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_join.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
