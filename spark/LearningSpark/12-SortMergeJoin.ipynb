{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/25 20:11:59 WARN Utils: Your hostname, wedivv-H110M-S2V resolves to a loopback address: 127.0.1.1; using 192.168.1.44 instead (on interface wlp5s0)\n",
      "23/05/25 20:11:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/25 20:12:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = ( SparkSession\n",
    "         .builder\n",
    "         .appName(\"SortMergeJoin\")\n",
    "         .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {0: \"AZ\", 1: \"CO\", 2: \"CA\", 3: \"TX\", 4: \"NY\", 5: \"MI\"}\n",
    "items = {0: \"SKU-0\", 1: \"SKU-1\", 2: \"SKU-2\", 3: \"SKU-3\", 4: \"SKU-4\", 5: \"SKU-5\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data = [(id, f\"user_{id}\", f\"user_{id}@databricks.com\", random.choice(states))\n",
    "              for id in range(1000001)]\n",
    "\n",
    "usersDF = spark.createDataFrame(users_data, [\"uid\", \"login\", \"email\", \"user_state\"]).repartition(\"uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_data = [(r, r, random.randint(0, 9999), 10 * r * 0.2, random.choice(states), random.choice(items))\n",
    "               for r in range(1000001)]\n",
    "\n",
    "ordersDF = spark.createDataFrame(orders_data, [\"transaction_id\", \"quantity\", \"users_id\", \"amount\", \"state\", \"items\"]).repartition(\"users_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersOrdersDF = ordersDF.join(usersDF, ordersDF.users_id == usersDF.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/25 20:12:56 WARN TaskSetManager: Stage 0 contains a task of very large size (6839 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/25 20:12:58 WARN TaskSetManager: Stage 1 contains a task of very large size (12604 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 1:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------+-----+-----+---+------+---------------------+----------+\n",
      "|transaction_id|quantity|users_id|amount  |state|items|uid|login |email                |user_state|\n",
      "+--------------+--------+--------+--------+-----+-----+---+------+---------------------+----------+\n",
      "|5790          |5790    |0       |11580.0 |CO   |SKU-5|0  |user_0|user_0@databricks.com|TX        |\n",
      "|7482          |7482    |0       |14964.0 |NY   |SKU-0|0  |user_0|user_0@databricks.com|TX        |\n",
      "|10647         |10647   |0       |21294.0 |AZ   |SKU-2|0  |user_0|user_0@databricks.com|TX        |\n",
      "|30588         |30588   |0       |61176.0 |TX   |SKU-1|0  |user_0|user_0@databricks.com|TX        |\n",
      "|31361         |31361   |0       |62722.0 |CA   |SKU-1|0  |user_0|user_0@databricks.com|TX        |\n",
      "|36367         |36367   |0       |72734.0 |CA   |SKU-2|0  |user_0|user_0@databricks.com|TX        |\n",
      "|45868         |45868   |0       |91736.0 |CO   |SKU-5|0  |user_0|user_0@databricks.com|TX        |\n",
      "|49631         |49631   |0       |99262.0 |CO   |SKU-4|0  |user_0|user_0@databricks.com|TX        |\n",
      "|53566         |53566   |0       |107132.0|TX   |SKU-1|0  |user_0|user_0@databricks.com|TX        |\n",
      "|59156         |59156   |0       |118312.0|CO   |SKU-4|0  |user_0|user_0@databricks.com|TX        |\n",
      "|80467         |80467   |0       |160934.0|AZ   |SKU-1|0  |user_0|user_0@databricks.com|TX        |\n",
      "|81844         |81844   |0       |163688.0|TX   |SKU-4|0  |user_0|user_0@databricks.com|TX        |\n",
      "|93051         |93051   |0       |186102.0|CO   |SKU-0|0  |user_0|user_0@databricks.com|TX        |\n",
      "|94876         |94876   |0       |189752.0|NY   |SKU-0|0  |user_0|user_0@databricks.com|TX        |\n",
      "|95390         |95390   |0       |190780.0|CO   |SKU-0|0  |user_0|user_0@databricks.com|TX        |\n",
      "|97604         |97604   |0       |195208.0|TX   |SKU-0|0  |user_0|user_0@databricks.com|TX        |\n",
      "|98770         |98770   |0       |197540.0|NY   |SKU-2|0  |user_0|user_0@databricks.com|TX        |\n",
      "|110369        |110369  |0       |220738.0|NY   |SKU-5|0  |user_0|user_0@databricks.com|TX        |\n",
      "|130907        |130907  |0       |261814.0|CO   |SKU-1|0  |user_0|user_0@databricks.com|TX        |\n",
      "|134807        |134807  |0       |269614.0|AZ   |SKU-5|0  |user_0|user_0@databricks.com|TX        |\n",
      "+--------------+--------+--------+--------+-----+-----+---+------+---------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "usersOrdersDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [users_id#10L], [uid#0L], Inner\n",
      "   :- Sort [users_id#10L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(users_id#10L, 200), REPARTITION_BY_COL, [plan_id=143]\n",
      "   :     +- Filter isnotnull(users_id#10L)\n",
      "   :        +- Scan ExistingRDD[transaction_id#8L,quantity#9L,users_id#10L,amount#11,state#12,items#13]\n",
      "   +- Sort [uid#0L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(uid#0L, 200), REPARTITION_BY_COL, [plan_id=146]\n",
      "         +- Filter isnotnull(uid#0L)\n",
      "            +- Scan ExistingRDD[uid#0L,login#1,email#2,user_state#3]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersOrdersDF.explain()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing the shuffle sort merge join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS UsersTbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/25 20:13:01 WARN TaskSetManager: Stage 5 contains a task of very large size (12604 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    usersDF.orderBy(asc(\"uid\"))\n",
    "    .write.format(\"parquet\")\n",
    "    .bucketBy(8, \"uid\")\n",
    "    .mode('overwrite')\n",
    "    .saveAsTable(\"UsersTbl\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/25 20:13:07 WARN TaskSetManager: Stage 13 contains a task of very large size (6839 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "ordersDF.orderBy(asc(\"users_id\"))\n",
    "    .write.format(\"parquet\")\n",
    "    .bucketBy(8, \"users_id\")\n",
    "    .mode('Overwrite')\n",
    "    .saveAsTable(\"OrdersTbl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CACHE TABLE UsersTbl\")\n",
    "spark.sql(\"CACHE TABLE OrdersTbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read them back in\n",
    "usersBucketDF = spark.table(\"UsersTbl\")\n",
    "ordersBucketDF = spark.table(\"OrdersTbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinUsersOrdersBucketDF = ordersBucketDF \\\n",
    "    .join(usersBucketDF, ordersBucketDF[\"users_id\"] == usersBucketDF[\"uid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------+-----+-----+---+------+---------------------+----------+\n",
      "|transaction_id|quantity|users_id|amount  |state|items|uid|login |email                |user_state|\n",
      "+--------------+--------+--------+--------+-----+-----+---+------+---------------------+----------+\n",
      "|16668         |16668   |2       |33336.0 |MI   |SKU-3|2  |user_2|user_2@databricks.com|NY        |\n",
      "|35005         |35005   |2       |70010.0 |CA   |SKU-0|2  |user_2|user_2@databricks.com|NY        |\n",
      "|37983         |37983   |2       |75966.0 |CO   |SKU-3|2  |user_2|user_2@databricks.com|NY        |\n",
      "|46158         |46158   |2       |92316.0 |TX   |SKU-3|2  |user_2|user_2@databricks.com|NY        |\n",
      "|52445         |52445   |2       |104890.0|CO   |SKU-0|2  |user_2|user_2@databricks.com|NY        |\n",
      "|54882         |54882   |2       |109764.0|AZ   |SKU-0|2  |user_2|user_2@databricks.com|NY        |\n",
      "|86650         |86650   |2       |173300.0|NY   |SKU-5|2  |user_2|user_2@databricks.com|NY        |\n",
      "|96844         |96844   |2       |193688.0|CA   |SKU-4|2  |user_2|user_2@databricks.com|NY        |\n",
      "|120801        |120801  |2       |241602.0|AZ   |SKU-3|2  |user_2|user_2@databricks.com|NY        |\n",
      "|127512        |127512  |2       |255024.0|TX   |SKU-3|2  |user_2|user_2@databricks.com|NY        |\n",
      "|128728        |128728  |2       |257456.0|CO   |SKU-4|2  |user_2|user_2@databricks.com|NY        |\n",
      "|139618        |139618  |2       |279236.0|CA   |SKU-2|2  |user_2|user_2@databricks.com|NY        |\n",
      "|192613        |192613  |2       |385226.0|CA   |SKU-0|2  |user_2|user_2@databricks.com|NY        |\n",
      "|202803        |202803  |2       |405606.0|NY   |SKU-0|2  |user_2|user_2@databricks.com|NY        |\n",
      "|203216        |203216  |2       |406432.0|AZ   |SKU-2|2  |user_2|user_2@databricks.com|NY        |\n",
      "|206281        |206281  |2       |412562.0|TX   |SKU-2|2  |user_2|user_2@databricks.com|NY        |\n",
      "|236063        |236063  |2       |472126.0|AZ   |SKU-5|2  |user_2|user_2@databricks.com|NY        |\n",
      "|238195        |238195  |2       |476390.0|AZ   |SKU-3|2  |user_2|user_2@databricks.com|NY        |\n",
      "|239299        |239299  |2       |478598.0|MI   |SKU-1|2  |user_2|user_2@databricks.com|NY        |\n",
      "|255238        |255238  |2       |510476.0|TX   |SKU-0|2  |user_2|user_2@databricks.com|NY        |\n",
      "+--------------+--------+--------+--------+-----+-----+---+------+---------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinUsersOrdersBucketDF.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6sec vs 0.6sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [users_id#230L], [uid#91L], Inner\n",
      "   :- Sort [users_id#230L ASC NULLS FIRST], false, 0\n",
      "   :  +- Filter isnotnull(users_id#230L)\n",
      "   :     +- Scan In-memory table OrdersTbl [transaction_id#228L, quantity#229L, users_id#230L, amount#231, state#232, items#233], [isnotnull(users_id#230L)]\n",
      "   :           +- InMemoryRelation [transaction_id#228L, quantity#229L, users_id#230L, amount#231, state#232, items#233], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                 +- *(1) ColumnarToRow\n",
      "   :                    +- FileScan parquet spark_catalog.default.orderstbl[transaction_id#228L,quantity#229L,users_id#230L,amount#231,state#232,items#233] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/wedivv/Code/others/zDEJourney/spark/LearningSpark/spark-war..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:bigint,quantity:bigint,users_id:bigint,amount:double,state:string,items:str..., SelectedBucketsCount: 8 out of 8\n",
      "   +- Sort [uid#91L ASC NULLS FIRST], false, 0\n",
      "      +- Filter isnotnull(uid#91L)\n",
      "         +- Scan In-memory table UsersTbl [uid#91L, login#92, email#93, user_state#94], [isnotnull(uid#91L)]\n",
      "               +- InMemoryRelation [uid#91L, login#92, email#93, user_state#94], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) ColumnarToRow\n",
      "                        +- FileScan parquet spark_catalog.default.userstbl[uid#91L,login#92,email#93,user_state#94] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/wedivv/Code/others/zDEJourney/spark/LearningSpark/spark-war..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<uid:bigint,login:string,email:string,user_state:string>, SelectedBucketsCount: 8 out of 8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinUsersOrdersBucketDF.explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
