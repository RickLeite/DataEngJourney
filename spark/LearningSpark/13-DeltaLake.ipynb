{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/10 15:13:44 WARN Utils: Your hostname, wedivv-H110M-S2V resolves to a loopback address: 127.0.1.1; using 192.168.1.44 instead (on interface wlp5s0)\n",
      "23/06/10 15:13:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/wedivv/spark/spark-3.4.0-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/wedivv/.ivy2/cache\n",
      "The jars for the packages stored in: /home/wedivv/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-98e6c6e7-9ac5-4742-86d1-e211b9e75f09;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in local-m2-cache\n",
      ":: resolution report :: resolve 173ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-98e6c6e7-9ac5-4742-86d1-e211b9e75f09\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "23/06/10 15:13:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"Delta Lake Loans\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcePath = \"./data/13-loan-risks.snappy.parquet\"\n",
    "\n",
    "# Delta Lake path\n",
    "deltaPath = \"./data/tmp/loans_delta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/10 15:13:56 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create the Delta table with the same loans data\n",
    "\n",
    "( \n",
    "    spark\n",
    "    .read\n",
    "    .format(\"parquet\")\n",
    "    .load(sourcePath)\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .save(deltaPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view on the data called loans_delta\n",
    "\n",
    "(\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .load(deltaPath)\n",
    "    .createOrReplaceTempView(\"loans_delta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:============================================>           (40 + 4) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   14705|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM loans_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|\n",
      "+-------+-----------+---------+----------+\n",
      "|      0|       1000|   182.22|        CA|\n",
      "|      1|       1000|   361.19|        WA|\n",
      "|      2|       1000|   176.26|        TX|\n",
      "|      3|       1000|   1000.0|        OK|\n",
      "|      4|       1000|   249.98|        PA|\n",
      "|      5|       1000|    408.6|        CA|\n",
      "|      6|       1000|   1000.0|        MD|\n",
      "|      7|       1000|   168.81|        OH|\n",
      "|      8|       1000|   193.64|        TX|\n",
      "|      9|       1000|   218.83|        CT|\n",
      "|     10|       1000|   322.37|        NJ|\n",
      "|     11|       1000|   400.61|        NY|\n",
      "|     12|       1000|   1000.0|        FL|\n",
      "|     13|       1000|   165.88|        NJ|\n",
      "|     14|       1000|    190.6|        TX|\n",
      "|     15|       1000|   1000.0|        OH|\n",
      "|     16|       1000|   213.72|        MI|\n",
      "|     17|       1000|   188.89|        MI|\n",
      "|     18|       1000|   237.41|        CA|\n",
      "|     19|       1000|   203.85|        CA|\n",
      "+-------+-----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM loans_delta\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforcing Schema on Write to Prevent Data Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# the column closed doesn't exist in the loans_delta table\n",
    "cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state', 'closed']\n",
    "\n",
    "items = [\n",
    "(1111111, 1000, 1000.0, 'TX', True),\n",
    "(2222222, 2000, 0.0, 'CA', False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanUpdates = (\n",
    "    spark.createDataFrame(items, cols)\n",
    "    .withColumn(\"funded_amnt\", col(\"funded_amnt\").cast(\"int\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 6afec9b8-64ed-4ee2-af87-5eca91c24434).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n\n\nData schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- closed: boolean (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m (\n\u001b[1;32m      2\u001b[0m     loanUpdates\n\u001b[1;32m      3\u001b[0m     \u001b[39m.\u001b[39;49mwrite\n\u001b[1;32m      4\u001b[0m     \u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mdelta\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m     \u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39mappend\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[39m.\u001b[39;49msave(deltaPath)\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/spark/spark-3.4.0-bin-hadoop3/python/pyspark/sql/readwriter.py:1398\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[1;32m   1397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1398\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark/spark-3.4.0-bin-hadoop3/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 6afec9b8-64ed-4ee2-af87-5eca91c24434).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n\n\nData schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- closed: boolean (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "(\n",
    "    loanUpdates\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(deltaPath)\n",
    ")\n",
    "\n",
    "#   \"name\": \"AnalysisException\",\n",
    "#   \"message\": \"A schema mismatch detected when writing to the Delta table \n",
    "#  To enable schema migration using DataFrameWriter or DataStreamWriter, \n",
    "#  please set:\\n'.option(\\\"mergeSchema\\\", \\\"true\\\")'.\\n\n",
    "#  For other operations, set the session configuration\\nspark.databricks.delta.schema.autoMerge.enabled to \\\"true\\\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolving Schemas to Accommodate Changing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    loanUpdates\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .save(deltaPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n",
      "+-------+-----------+---------+----------+------+\n",
      "|      0|       1000|   182.22|        CA|  null|\n",
      "|      1|       1000|   361.19|        WA|  null|\n",
      "|      2|       1000|   176.26|        TX|  null|\n",
      "|      3|       1000|   1000.0|        OK|  null|\n",
      "|      4|       1000|   249.98|        PA|  null|\n",
      "|      5|       1000|    408.6|        CA|  null|\n",
      "|      6|       1000|   1000.0|        MD|  null|\n",
      "|      7|       1000|   168.81|        OH|  null|\n",
      "|      8|       1000|   193.64|        TX|  null|\n",
      "|      9|       1000|   218.83|        CT|  null|\n",
      "|     10|       1000|   322.37|        NJ|  null|\n",
      "|     11|       1000|   400.61|        NY|  null|\n",
      "|     12|       1000|   1000.0|        FL|  null|\n",
      "|     13|       1000|   165.88|        NJ|  null|\n",
      "|     14|       1000|    190.6|        TX|  null|\n",
      "|     15|       1000|   1000.0|        OH|  null|\n",
      "|     16|       1000|   213.72|        MI|  null|\n",
      "|     17|       1000|   188.89|        MI|  null|\n",
      "|     18|       1000|   237.41|        CA|  null|\n",
      "|     19|       1000|   203.85|        CA|  null|\n",
      "+-------+-----------+---------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(deltaPath)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "\n",
    "deltaTable.update(\"addr_state = 'OR'\", {\"addr_state\": \"'WA'\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "\n",
    "deltaTable.delete(\"funded_amnt >= paid_amnt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upserting change data to a table using merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# say we have another table of new loan information, some of which\n",
    "# are new loans and others of which are updates to existing loans.\n",
    "# ( and same schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    deltaTable\n",
    "    .alias(\"t\")\n",
    "    .merge(loanUpdates.alias(\"s\"), \"t.loan_id = s.loan_id\")\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "# DeltaTable.merge() operation, which is based on the MERGE SQL command"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditing Data Changes with Operation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      4|2023-06-10 15:29:...|  null|    null|    MERGE|{predicate -> [\"(...|null|    null|     null|          3|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.4....|\n",
      "|      3|2023-06-10 15:26:...|  null|    null|   DELETE|{predicate -> [\"(...|null|    null|     null|          2|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|      2|2023-06-10 15:26:...|  null|    null|   UPDATE|{predicate -> [\"(...|null|    null|     null|          1|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|      1|2023-06-10 15:20:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          0|  Serializable|         true|{numFiles -> 3, n...|        null|Apache-Spark/3.4....|\n",
      "|      0|2023-06-10 15:13:...|  null|    null|    WRITE|{mode -> ErrorIfE...|null|    null|     null|       null|  Serializable|         true|{numFiles -> 1, n...|        null|Apache-Spark/3.4....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|version|timestamp              |operation|operationParameters                                                                                                                                                                     |\n",
      "+-------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|4      |2023-06-10 15:29:14.007|MERGE    |{predicate -> [\"(loan_id#2409L = loan_id#811L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|\n",
      "|3      |2023-06-10 15:26:12.7  |DELETE   |{predicate -> [\"(cast(funded_amnt#2410 as double) >= paid_amnt#2411)\"]}                                                                                                                 |\n",
      "|2      |2023-06-10 15:26:04.716|UPDATE   |{predicate -> [\"(addr_state#1616 = OR)\"]}                                                                                                                                               |\n",
      "+-------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    deltaTable\n",
    "    .history(3)\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationParameters\")\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Previous Snapshots of a Table with Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n",
      "+-------+-----------+---------+----------+------+\n",
      "|1111111|       1000|   1000.0|        TX|  true|\n",
      "|2222222|       2000|      0.0|        CA| false|\n",
      "+-------+-----------+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(    \n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .load(deltaPath)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n",
      "+-------+-----------+---------+----------+------+\n",
      "|      0|       1000|   182.22|        CA|  null|\n",
      "|      1|       1000|   361.19|        WA|  null|\n",
      "|      2|       1000|   176.26|        TX|  null|\n",
      "|      3|       1000|   1000.0|        OK|  null|\n",
      "|      4|       1000|   249.98|        PA|  null|\n",
      "|      5|       1000|    408.6|        CA|  null|\n",
      "|      6|       1000|   1000.0|        MD|  null|\n",
      "|      7|       1000|   168.81|        OH|  null|\n",
      "|      8|       1000|   193.64|        TX|  null|\n",
      "|      9|       1000|   218.83|        CT|  null|\n",
      "|     10|       1000|   322.37|        NJ|  null|\n",
      "|     11|       1000|   400.61|        NY|  null|\n",
      "|     12|       1000|   1000.0|        FL|  null|\n",
      "|     13|       1000|   165.88|        NJ|  null|\n",
      "|     14|       1000|    190.6|        TX|  null|\n",
      "|     15|       1000|   1000.0|        OH|  null|\n",
      "|     16|       1000|   213.72|        MI|  null|\n",
      "|     17|       1000|   188.89|        MI|  null|\n",
      "|     18|       1000|   237.41|        CA|  null|\n",
      "|     19|       1000|   203.85|        CA|  null|\n",
      "+-------+-----------+---------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .option(\"versionAsOf\", \"2\")\n",
    "    .load(deltaPath)\n",
    "    ).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
